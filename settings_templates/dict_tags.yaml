# The 'tags' feature is a versatile method to tailor the bot's behavior for your use case.
# tags may be defined in this file for global effect, as well as in character cards and imgmodel definitions.

# Notes:
# - 'trigger' key is recommended for most tags (except 'global_tag_keys' and 'tag_preset_name').
# - Multiple comma-separated trigger phrases can be used per each tag (trigger: 'a boy,a dog' can match either 'a boy' or 'a dog')
# - You may simplify triggers using this syntax: 'a {boy|dog}' --- the script will read this as 'a boy,a dog'
# - You may use multiple trigger keys in a tag definition such as 'trigger1', 'trigger2' - in which case, a trigger phrase must match the search text from each one
# - The behavior of tags listed higher may override ones listed lower. Sources are also prioritized: character > imgmodel > base_tags
# - Valid tag keys are detailed at the end of this file and on GitHub

# 'global_tag_keys' apply to ALL tag definitions. Keys defined elsewhere will override global tag keys.
global_tag_keys:
  search_mode: userllm            # 'user' = search user prompt / 'llm' = search bot reply / 'userllm' = search both
  case_sensitive: false           # If triggers are case sensitive
  text_joining: ' '               # Relevant to 'insert_text'. Method of joining insert_text to text prompt
  img_text_joining: ', '          # Relevant to 'positive_prompt' (and '_prefix' / '_suffix'). Method of joining the positive_prompt to the img prompt
  positive_prompt_method: after   # Relevant in tags with key 'positive_prompt'. 'after' = insert value after matched text / 'before' = before text / 'replace' = replace text

# 'base_tags' are always in effect. Base tags have lower priority than tags defined anywhere else.
base_tags:
  # Use {time} or {date} written anywhere in your prompts, which will be replaced with the current date/time. The tags 'offset' and '(date/time)_format' will apply to the result.
  - time_offset: 0.0              # 0 = today's date (system time). -0.5 shifts the current date to be 12 hours ago. (UOM = days).
    time_format: '%H:%M:%S'       # Examples: '%H:%M:%S' (HH:MM:SS), '%I:%M:%S %p' (12HR:MM:SS AM/PM)
    date_format: '%Y-%m-%d'       # Examples: '%Y-%m-%d' (YYYY-MM-DD), '%A, %B %d, %Y' (DayName, MonthName DD, YYYY)

# Stable Diffusion payload and textgen-webui params can be randomly modified within specified ranges, or a list of strings (ex: sampler names), or from a boolean value.
# These examples are commented out by default - you should uncomment and experiment :D
#  - img_param_variances: {cfg_scale: [-2,4], steps: [-5,5], hr_scale: [-0.50,0.15], hr_enable: false, sampler_name: ['Euler A', 'DPM++ 2M', 'DPM++ 2M']}
#  - llm_param_variances: {repetition_penalty: [-0.25,0.25], temperature_last: false}

  # Tag for image censoring feature
  - trigger: 'nude,erotic'
    llm_censoring: true           # true = blocks text from being generated
    img_censoring: 0              # 0 = disabled / 1 = blur image / 2 = block image from being generated

  # Tag to trigger Instruct mode with better reasoning params
  - trigger: 'calculate,define'
    on_prefix_only: true
    state: {preset: 'Divine Intellect', mode: instruct} # Update values for textgen-webui state parameters.

  # Example usage for 'call_api' tag (see 'dict_api_settings.yaml')
  - trigger: dim the lights
    call_api:
      api_name: HomeAPI v3  # or 'api_type' for a 'main API' (imggen/ttsgen/textgen)
      endpoint_name: Lights # or 'endpoint_type' for a 'main API' (post_txt2img, etc)
      input_data: {brightness: 0.5, hue: amber} # Updates any base payload configured for endpoint
      queue_to: null # null = runs immediately.  Defaults to 'gen_queue' (creates/queues a new 'Task'). Other valid options are 'history_queue' (tasks that modify chat history) and 'normal_queue' (light tasks)

  # Example usage for 'run_workflow' tag (see 'dict_api_settings.yaml')
  - trigger: generate a video of
    on_prefix_only: true
    insert_text: ''
    insert_text_method: 'replace'
    run_workflow:
      name: Wan t2v # the name of a workflow
      input_data: {prompt: '{prompt}'} # Example using bot formatting syntax to format the user's text into the prompt field
      queue_to: gen_queue # defaults to gen_queue if 'queue_to' is omitted

  # Tag that triggers a 'Flow' which asks 'Imgmodel_Selector' to review the prompt
  #    which was written by 'M1inty' to assign the best Image Model for the prompt.
  - trigger: 'check imgmodel'                   # Intended to be used with a prompt like 'draw _______ check imgmodel'
    insert_text: ''
    insert_text_method: 'replace'
    search_mode: user                           # Only search user's prompt (will not trigger from LLM reply)
    should_send_text: false                     # Suppress text response (still generate text)
    should_gen_image: false                     # Suppress image generation (won't send an image by default)
    flow:                                       # The actual 'Flow' tag value
      - flow_base:                              # Flow_base are tags that apply to every flow_step
          save_history: False                   # Do not write these interactions to the main chat history
          load_history: -1                      # 0 = default (all history), -1 = excludes chat history, > 1 = LLM sees this many recent exchanges.
      - flow_step: Ask LLM for best Image Model # text displayed in discord embed
        format_prompt: '{llm_0}'                # Replace prompt with the most recent LLM reply (from M1nty - the image prompt)
        swap_character: Imgmodel_Selector       # Swap to character finetuned for choosing Image models
        should_send_text: false                 # Suppress text response
        should_gen_image: false                 # Suppress image generation
      - flow_step: Gen image with LLM's chosen Img Model
        format_prompt: '{llm_1}'                # Replace prompt with the 2nd most recent LLM reply (from M1nty - the image prompt)
        change_imgmodel: '{llm_0}'              # Use the reply from Imgmodel_Selector
        should_gen_image: true                  # Generate image
        should_gen_text: false                  # ** IF NO TEXT GEN - the prompt will be used directly for Image Generation **
        should_send_text: true                  # Send the prompt to channel

  # Tag that triggers a 'Flow' which asks Prompt_Enhancer_XL repeatedly to add more details to it's previous reply.
  - trigger: 'loopback'                         # Intended to be used with a prompt like 'draw _______ loopback'
    insert_text: ''
    insert_text_method: 'replace'
    search_mode: user                           # Only search user's prompt (will not trigger from LLM reply)
    should_send_text: false
    should_gen_image: false
    flow:
      - flow_base:
          format_prompt: 'Provide a more detailed version of this prompt: {llm_0}'
          save_history: False
          load_history: -1
          swap_character: Prompt_Enhancer_XL
          should_send_text: false
          should_gen_image: false
      - flow_step: Request more details
        flow_step_loops: 3                      # this flow_step will execute 3 times (reminder: includes flow_base)
        should_gen_image: false
      - flow_step: Gen with extra detailed prompt
        should_gen_image: true                  # finally, generate the image
        should_send_text: true                  # finally, send the reply to channel

  # Tag to trigger M1nty to make a more elaborate prompt from user's prompt
  - name: 'img gen Prompt_Enhancer_XL'
    trigger: 'draw,generate'
    insert_text: ''               # Replace the matched phrase with an empty string
    insert_text_method: replace   # Other methods are 'before' and 'after', but 'replace' is correct for this...
    search_mode: user             # Only search user's prompt (will not trigger from LLM reply)
    on_prefix_only: true          # Only triggers if matched at very beginning
    should_gen_image: true        # Triggers an image generation (will send to channel afterwards by default - cann be suppressed with 'should_send_image' tag)
    swap_character: Prompt_Enhancer_XL    # Swap to character finetuned for writing more elaborate image prompts from simple prompts
    load_history: -1              # 0 = default (all history included) / -1 = prompt excludes chat history / > 1 = llm only sees this many recent exchanges.
    save_history: false           # Do not write this interaction to the main chat history

  - name: Example using 'only_with_tags' condition
    only_with_tags: ['img gen Prompt_Enhancer_XL', 'another tag name'] # Condition that is True if any of the listed tag names are matched
    # Does nothing. Just for example usage of the parameter.

  # Some tags to trigger/enhance image responses using the current character including chat history
  - trigger: 'take a photo,take {a|another} picture'
    search_mode: user
    should_gen_image: true
    format_prompt: '''[SYSTEM] You have been tasked with creating a text-to-image prompt. Describe the scene in concise and vivid detail: "{prompt}"'''
  - trigger: 'selfie,self portrait'
    search_mode: user
    should_gen_image: true
    format_prompt: '''[SYSTEM] You have been tasked with taking a selfie. Include your appearance, your surroundings, and what you are doing right now: "{prompt}"'''

  # Example of a tag for triggering SD Forge Couple extension (if enabled in config.py)
  - trigger: forgewide # you could also make another such as 'forgetall' to set 'forge_couple: Vertical'
    insert_text: ''
    insert_text_method: 'replace'
    forge_couple: Horizontal
    couple_sep: 'SEP' # Separator for prompting regions. They are assigned in order from left to right (Horizontal) and top to bottom (Vertical).
    sd_output_dir: 'sd_outputs/forge_couple/' # custom output directory


# 'tag_presets' will be imported anywhere you use a 'tag_preset_name' tag.
# Useful for when you want to use the same set of tags for mulitple uses (a few characters, a few imgmodels, etc).
# Example: '- tag_preset_name: SDXL Tags' would be a simple way to share tags among all your SDXL imgmodels defined in dict_imgmodels.yaml.
tag_presets:
  - tag_preset_name: Flux Payload
    tags:
      - aspect_ratio: '1:1'
      - payload: {cfg_scale: 1, distilled_cfg_scale: 3.5, steps: 30, sampler_name: 'Euler', scheduler: 'simple'}

  - tag_preset_name: SDXL Turbo Payload   # Each tag preset must be named
    tags:                                 # Tag presets must be nested under a 'tags' key
      - aspect_ratio: '3:4'
      - payload: {cfg_scale: 2, steps: 5, sampler_name: 'DPM++ SDE', scheduler: karras} # Stable Diffusion payload can be modified using 'payload'. Must be formatted as a dictionary.
      #- img_param_variances: {cfg_scale: [-0.5,0.5], steps: [-1,1], hr_scale: [-0.1,0.1], hr_enabled: true}

  - tag_preset_name: SDXL Payload
    tags:
      - aspect_ratio: '3:4'
      - payload: {cfg_scale: 7, steps: 25, sampler_name: 'DPM++ 2M',  scheduler: karras, hr_scale: 1.2}
      #- img_param_variances: {cfg_scale: [-2,3], steps: [0,10], hr_scale: [-0.1,0.1], hr_enabled: true}

  - tag_preset_name: SDXL Tags
    tags:
      - trigger: 'vertical,selfie,self {photo|portrait}'
      - aspect_ratio: '3:4'

      - trigger: 'horizontal,landscape'
      - aspect_ratio: '4:3'

      - trigger: 'plain'
        payload: {override_settings: {CLIP_stop_at_last_layers: 2}} # Nested payload dictionary settings can also be modified. Must be formatted as a dictionary.

      # Tag to trigger layerdiffuse (Forge extension)
      - trigger: '{no|without|clear|empty|transparent} {bg|background}'
        layerdiffuse: '(SDXL) Only Generate Transparent Image (Attention Injection)'  # See comments at bottom

      # Some examples for triggering LORAs
      - trigger: 'grimace'
        positive_prompt: '<lora:PE_Grimace_v1.0:0.75>'

      - trigger: '{ps1|playstation} {style|art|graphics},style of {ps1|playstation}'
        positive_prompt: 'ps1 style <lora:ps1_style_SDXL_v2:1.0>'
        negative_prompt: 'blurry'
        trumps: 'low poly' # If any other matched tag has a trigger phrase matching a 'trump' phrase, it will not be used. (*can be comma separated to trump multiple tags*)

      - trigger: 'low {poly|polygon}'
        positive_prompt: '<lora:LowpolySDXL_v1.0:1.0>'
        negative_prompt: 'blurry'

  - tag_preset_name: SD15 Tags
    tags:
      - aspect_ratio: '1:1'
      - payload: {cfg_scale: 10, steps: 40, sampler_name: 'DPM++ 2M', scheduler: karras, hr_scale: 2, hr_enabled: true, denoise_strength: 0.5}
      #- img_param_variances: {cfg_scale: [-3,1], steps: [-10,10], hr_scale: [-0.5,0.2]}

      - positive_prompt_prefix: 'masterpiece, detailed, '                                                 # '_prefix' makes this inserted at the beginning of the image prompt
        positive_prompt_suffix: ' <lora:more_details:0.5> <lora:epiNoiseoffset_v2Pynoise:0.5>'            # '_suffix' makes this inserted at the end of the image prompt
        negative_prompt_prefix: '<kkw-Extreme-Neg:0.5>, <negative_hand-neg:0.5>, by <bad-artist:0.5>, '   # same logic as positive_prompt. Can similarly use '_suffix' (or just 'negative_prompt')

      - trigger: 'selfie,self photo'
        positive_prompt: '(taking a selfie:1.2), (arms outstretched:1.1)'

      - trigger: 'gta,gta4,grand theft auto,archer style'
        positive_prompt: 'GtaSA2004 cartoon <lora:GTA_Style:1.0>'

      - trigger: 'gigachad'
        positive_prompt: '<lora:Gigachadv1:0.8>'


# List of all available tag keys:
#
# Conditional tags:
#   search_mode: userllm              Search method for trigger matching. 'user' = user prompt only / 'llm' = bot reply only / 'userllm' = search both for matches
#   trigger: 'trigger,another one'    Recommended for most tags (except 'global_tag_keys' and 'tag_preset_name').
#     Additional triggers (trigger1, trigger2, etc) - if using multiple trigger keys, a trigger phrase must match the search text from each one
#   only_with_tags: [list of strings] A list of tag names (see 'name' param). This condition will be True if any named tag was matched. (ex, only_with_tags: ['imgmodel', ''])
#   random: 0.5                       Chance for tag to process. 0.0 = 0% chance and 1.0 = 100% chance.
#   case_sensitive: (true/false)      If the triggers are case sensitive
#   on_prefix_only: (true/false)      If triggers must be matched at beginning of the searched text
#                                     To access the "id" needed for the following conditions, you must enable "Developer Mode" for your account (Account Settings > Advanced > Developer Mode).
#                                     These can be either one id, example: int / or a comma separated list, example: [int, int, int]
#   for_guild_ids_only: int           Value is a 'server_id' number. Get this by right-clicking a Server and selecting "Copy Server ID".
#   for_channel_ids_only: int         Value is a 'channel_id' number. Get this by right-clicking a channel (or DM) and selecting "Copy Channel ID"
#   persist: int                      Tags matched with 'persist' will automatically re-apply for this number of future requests, in the same channel, during the same 'match phase'... before the tag is checked normally again.
#                                     Tag must include 'name' param. Ignores 'triggers', **but NOT 'random'**. 'persist: -1' will make triggered tag persist forever.

# Relevant to both text/img generation:
#   name: 'string'                    Used in print statements. Required when using a 'persist' tag.
#   send_user_image: 'string'         Sends a local image to the channel. Value values: Image in 'user_images' (png/jpg/txt in base64) -OR- Directory name in '/user_images/'
#                                       (Picks a random image or random image from random subdirectory)
#   call_api: {dict}                  Refer to 'dict_api_settings.yaml'. Required: {endpoint_name: The Endpoint Name}. Recommended to include: {api_type: (imggen/ttsgen/textgen)} for 'main' APIs, or {api_name: The Client Name} for all others.
#                                       Any endpoint values can be overridden in this dict. Payload can be updated with {input_data: {key: value, ...}}.
#   run_workflow: {dict}              Refer to 'dict_api_settings.yaml'. Runs a predefined workflow. Valid usage: {name: Name of the Workflow, (optional)input_data: {key1: value1, key2: value2, etc}}.
#     For 'call_api' and 'run_workflow': Any string values can contain bot formtting syntax, ex: "{prompt}" will be replaced with the actual user's text.
#
#   flow: {dict}                      A dict of 'flow_steps'. When a 'flow' tag is triggered, it creates a message loop for each defined 'loop_step'
#   flow_step: [list]                 The tags you want to apply in each flow step.  Without a trigger, they will auto-apply.  'format_prompt' is very useful in each flow_step
#   flow_step_loops: int              The number of times you want to loop a particular flow_step.
#   toggle_vc_playback: str           Changes playback in guild's voice channel where tag is triggered. Use with 'for_guild_ids_only' condition for selective control. Valid values: 'stop', 'pause', 'resume', 'toggle' (pauses or resumes)

# Relevant to text generation (LLM):
#   should_gen_text: (true/false)     If LLM should generate text. If False and 'should_gen_image' is True, passes the user's text as prompt for image gen.
#   should_send_text: (true/false)    If LLM should send generated text to channel. Mainly used for 'flows' tag.
#   should_tts: (true/false)          false = prevents TTS from generating for the current interaction
#   llm_censoring: (true/false)       true = blocks text from being generated
#   begin_reply_with: 'string'        The LLM will continue their reply from this.
#   change_llmmodel: 'string'         For (change/swap)_llmmodel: 'change' will trigger a persistent change. 'swap' will only change the LLM model for the current interaction.
#   swap_llmmodel: 'string'           Name of an LLM model (ei: 'Mistral-7B-OpenOrca-GPTQ'). Value 'None' will unload the LLM model!
#   time_offset: 0.0                  0 = today's date (system time). -0.5 shifts the current date to be 12 hours ago. (UOM = days). Use with '{time}' anywhere in your prompts
#   time_format: '%H:%M:%S'           Examples: '%H:%M:%S' (HH:MM:SS), '%I:%M:%S %p' (12HR:MM:SS AM/PM)
#   date_format: '%Y-%m-%d'           Examples: '%Y-%m-%d' (YYYY-MM-DD), '%A, %B %d, %Y' (DayName, MonthName DD, YYYY)
#   format_prompt: 'string'           Useful for adding text before and/or after your prompt to the LLM. **If you do not include {prompt}, your prompt will be replaced entirely with the value of 'format_prompt'.** Can include any variables, see '/tips_and_info/Message Variables.txt'
#   insert_text: ''                   Text you may want to insert relative to the matched search text.
#   insert_text_method: replace       Relevant in tags with key 'pinsert_text'. 'after' = insert value after matched text / 'before' = before text / 'replace' = replace text
#   text_joining: ' '                 Relevant to all insertion methods except "replace"
#   change_character: 'string'        Filename of character in /characters (do not include extension). That character's name, context, and state parameters will be used in LLM payload.
#   swap_character: 'string'          For (change/swap)_character: 'change' will trigger a persistent change. 'swap' will only change the character for the current interaction.
#   filter_history_for: 'string'      Applies before other "history" tags. Filters the history based on the name of a user or a (character/swap character). Includes the user/assistant exchange for matched items.
#   load_history: 0                   0 = default (all history included) / -1 = prompt excludes chat history / > 1 = llm only sees this many recent exchanges.
#   save_history: (true/false)        Whether to save this interaction to history or not.
#   include_hidden_history: (true/false) If true, the history for this interaction will include ones previously hidden via 'save_history' tag, or toggled off via 'toggle as hidden' App Command
#   state: {dictionary}               Update values for textgen-webui state parameters. Must be formatted as a dictionary.
#   prefix_context: 'string'          Inserts text immediately before the 'context', joined by \n (new line). Multiple can accumulate.
#   suffix_context: 'string'          Inserts text immediately after the 'context', joined by \n (new line). Multiple can accumulate.
#   llm_param_variances: {dictionary} Randomization ranges for textgen-webui state parameters. Must be formatted as a dictionary.

# Relevant to img generation (A1111 / Forge):
#   sd_output_dir: 'string'           Location to save any generated images relative to output dir 'ad_discordbot/output/'. Default: ''
#   should_gen_image: (true/false)    If bot should generate an image using Stable Diffusion. The LLM's reply will be used as the img prompt, unless no LLM Model is loaded or 'should_gen_text' is False - in which case user message is prompt.
#   should_send_image: (true/false)   If bot should send generated image to channel. Mainly used for 'flows' tag.
#   change_imgmodel: 'string'         For (change/swap)_imgmodel: 'change' will trigger a persistent change. 'swap' will only change the Img model for the current interaction.
#   swap_imgmodel: 'string'           Copy/paste a checkpoint name *exactly* as it appears in A1111/Forge model list (ei: 'sdxl\epicrealismXL_v4Photoreal.safetensors [c772a1a690]').
#                                     Currently, API Img model unloading is bugged in both A1111/Forge so that is not an option here.
#   img_censoring: 0                  0 = disabled / 1 = blur image / 2 = block image from being generated
#   positive_prompt: 'string'         Text you may want to insert relative to the matched text in the image prompt.
#   positive_prompt_method: after     Relevant to 'positive_prompt'. 'after' = insert value after matched text / 'before' = before text / 'replace' = replace text
#   positive_prompt_prefix: 'string'  Insert text at the beginning of the image positive prompt
#   positive_prompt_suffix: 'string'  Insert text at the end of the image positive prompt
#   negative_prompt_prefix: 'string'  Insert text at the beginning of the image negative prompt
#   negative_prompt: 'string'         Insert text at the end of the image negative prompt ('_suffix' also works)
#   img_text_joining: ', '            Relevant to all insertion methods except "replace"
#   last_img_payload: (true, [list])  If value is 'true' (boolean), the last image payload will be applied. If a ['list', 'of', 'key_names'], it will use apply only those key values from the last img payload. Note: This Tag is applied before other payload related tags.
#   payload: {dictionary}             Replacements for Stable Diffusion payload. Must be formatted as a dictionary.
#   aspect_ratio: 'string'            This a shortcut to dynamically set the payload width/height dimensions based on the current average dimensions. Valid values examples: '4:3', '16:9', '2:3', etc. A special value 'from img2img' may be used, requiring a valid 'img2img' tag param to be triggered in tandem.
#   img_param_variances: {dictionary} Randomization ranges for Stable Diffusion payload parameters. Must be formatted as a dictionary.
#   img2img: 'string'                 Uses a local image as img2img input. Value values: Image in 'user_images' (png/jpg/txt in base64) -OR- Directory name in '/user_images/' (Picks a random image or random image from random subdirectory)
#   img2img_mask: 'string'            AKA Inpainting. Won't have any effect without img2img also triggered. Should be a greyscale image. Value values: Image in 'user_images' (png/jpg/txt in base64) -OR- Directory name in '/user_images/' (Picks a random image or random image from random subdirectory)

# HINT: tags can be created/activated on-demand if using this sytax in your prompt: [[key:value]] or [[key1:value1 | key2:value2]] etc
#       EXAMPLE (Single tag):     [[reactor_max_faces:6]]
#       EXAMPLE (Multiple tags):  [[reactor_save_original:True | laydiff_weight:0.5]]
#   ** This is a good way to add specific extension parameters on-demand!! **


# ControlNet specific tags (A1111/Forge extension):
#   controlnet: 'string'              Simplified tag to both **apply** ControlNet image/folder AND **enable** ControlNet.
#   controlnet#: 'string'             Optional syntax for multi-ControlNet where # represents the ControlNet unit. ex: 'controlnet0: guy.png', 'controlnet1: architecture', etc.
#                                     Value values: Image in 'user_images' (png/jpg/txt in base64) -OR- Directory name in '/user_images/' (Picks a random image or random image from random subdirectory)
#   cnet_{key}                        ANY key can be used (see dict_base_settings.yaml for complete list). A 'controlnet' tag must also be in effect when using these tags in order to properly assign the image.
#   cnet#_{key}: 'string'             Optional syntax for multi-ControlNet parameters where # represents the ControlNet unit. ex: 'cnet0_weight: 0.5', 'cnet1_guidance_end: 0.8', etc.
#   cnet#_mask: 'string'              The value for the 'cnet_mask_image' param will be handled exactly the same as the main controlnet (see writeup for 'controlnet' a few lines up). 'cnet#_mask_image' is also valid.
#   cnet#_control_type: 'string'      Optional. If provided: for any controlnet image value (including mask) that is not a specific filename (ei: its directory name), it will try looking for this filename.

# Forge Couple specific tags (Forge extension):
#   forgecouple: 'string' OR [list]   Simplified tag to both **apply** AND **enable** Forge Couple.
#                                     For Basic mode, use string: 'Horizontal' / 'Vertical'. For Advanced mode, use a list of lists (refer to dict_base_settings.yaml)
#   couple_{key}                      ANY key can be used (see dict_base_settings.yaml for complete list). Note: Forge Couple will not be enabled without a 'forgecouple' tag.

# layerdiffuse specific tags (Forge extension):
#   layerdiffuse: 'string'            Simplified tag to both **apply** AND **enable** layerdiffuse. Note: 'laydiffuse_method' works exactly the same.
#   laydiff_method: 'string'          Valid values for either 'layerdiffuse' or 'laydiff_method':
#                                         "(SD1.5) Only Generate Transparent Image (Attention Injection)"
#                                         "(SD1.5) From Foreground to Background (need batch size 2)"
#                                         "(SD1.5) From Background to Foreground (need batch size 2)"
#                                         "(SD1.5) Generate Everything Together (need batch size 3)"
#                                         "(SDXL) Only Generate Transparent Image (Attention Injection)"
#                                         "(SDXL) Only Generate Transparent Image (Conv Injection)"
#                                         "(SDXL) From Foreground to Blending"
#                                         "(SDXL) From Foreground and Blending to Background"
#                                         "(SDXL) From Background to Blending"
#                                         "(SDXL) From Background and Blending to Foreground"
#   laydiff_{key}                     ANY key can be used (see dict_base_settings.yaml for complete list). Note: layerdiffusion will not be enabled without either a 'layerdiffuse' or 'laydiff_method' tag.

# ReActor specific tags (A1111/Forge extension):
#   reactor: 'string'                 Simplified tag to both **apply** face image/folder/model AND **enable** ReActor.
#                                     Value values: Image in 'user_images' (png/jpg/txt in base64) -OR- Directory name in '/user_images/' (Picks a random image or random image from random subdirectory) -OR- Face model in '{SD_CLIENT}\models\reactor\faces' example 'M1nty.safetensors'
#   reactor_{key}                     ANY key can be used (see dict_base_settings.yaml for complete list).
#                                     Recommended to use in combination with 'reactor' tag - otherwise, you can't use "single face image method"
#                                     (technically, you could send a base64 string... and you would need to include 'reactor_enabled: True')
#   reactor_mask                      Value values: Image in 'user_images' (png/jpg/txt in base64), directory, etc. Should be a greyscale image, which will mask the face swap result.