# These are the baseline settings you may adjust to your liking.
# When you use bot commands like /character, /imgmodel, etc - those settings will have higher priority.
# Note that Img model settings defined here DO NOT get merged until the bot changes Img models (via '/imgmodel' cmd, or the 'auto change Img models' feature).

# To see the currently active settings, look in internal/activesettings.yaml (which is not intended to be edited).

# There are many settings here you probably don't need to tinker with.

behavior:
  reply_to_itself: 0.0                # 0.0 = never happens / 1.0 = always happens
  chance_to_reply_to_other_bots: 0.0  # Chance for bot to reply when other bots speak in main channel
  reply_to_bots_when_addressed: 0.0   # Chance for bot to reply when other bots mention it by name
  only_speak_when_spoken_to: true     # This value gets ignored if you're talking in the bot's main channel
  ignore_parentheses: true            # (Bot ignores you if you write like this)
  go_wild_in_channel: true            # Whether or not the bot will always reply in the main channel
  conversation_recency: 600           # How long (in seconds) a user is determined to be "in active conversation". Only applicable if 'go_wild_in_channel' is false

  # Streaming responses. Note: You may want to disable the 'history reactions' feature in 'config.yaml' (discord limits speed for processing reactions)
  chance_to_stream_reply: 0.5                 # Chance trigger sending partial message as it is generating. Range: 0.0 - 2.0. 0.0 = Never split reply / 1.0 = Splits very often / 2.0 = Always splits for any trigger
  stream_reply_triggers: ['\n\n', '\n', '.']  # If you may want to adjust triggers. default: ['\n', '.']

  # Settings to control whether the bot behaves as a computer program (default), or optionally make the bot behave more humanlike by throttling its responses.
  # Only affects "normal messaging" - not commands/features.
  maximum_typing_speed: -1            # Words per Minute. 80 = 80 WPM (good typist) / -1 = no restriction (default).
  # 'responsiveness' and 'msg_size_affects_delay' separately influence the weights for response timing. 'max_reply_delay' will never be exceeded.
  responsiveness: 1.0                 # Controls "how much the character is paying attention". 0.0 = Distracted human / 1.0 = Computer program.
  msg_size_affects_delay: false       # Will make the bot respond faster for shorter messages, and slower for longer messages.
  max_reply_delay: 30.0               # Maximum delay before bot begins to respond (-seconds-).

  # 'spontaneous_messaging' will select a random prompt from a list instead of being prompted by user message.
  # Note that syntax for features such as Variables, Dynamic Prompting, Tags from Text, etc, can all be used in these prompts
  spontaneous_msg_chance: 0.0         # 0.0 = never happens / 1.0 = always happens (guaranteed to happen prior to or at 'max_delay')
  spontaneous_msg_max_consecutive: 1  # Number of allowed spontaneous messages without User activity. -1 = Unlimited / other value like 10 would be 10 messages
  spontaneous_msg_min_wait: 10.0      # Minimum wait before bot may spontaneously message (-minutes-)
  spontaneous_msg_max_wait: 60.0      # Maximum wait before bot may spontaneously message (-minutes-)
  spontaneous_msg_prompts:            # Random prompt will be picked from this list if triggered
    - '''[SYSTEM] The conversation has been inactive for {time_since_last_msg}, so you should say something.'''
  # - 'another prompt' 

llmcontext:
  context: 'The following is a conversation with an AI Large Language Model. The AI has been trained to answer questions, provide recommendations, and help with decision making. The AI follows user requests. The AI thinks outside the box.'
  extensions: {}
  greeting: 'How can I help you today?'
  name: 'AI'
  use_voice_channel: false

llmstate:
  state:
    preset: ''  # Name of a file in '/presets' (ex. 'Midnight Enigma'). Preset settings will override values defined here.
    grammar_string: ''
    add_bos_token: true
    auto_max_new_tokens: false
    ban_eos_token: false
    character_menu: ''
    chat_generation_attempts: 1
    chat_prompt_size: 2048
    custom_stopping_strings: ''
    custom_system_message: ''
    custom_token_bans: ''
    do_sample: true
    dry_multiplier: 0
    dry_base: 1.75
    dry_allowed_length: 2
    dry_sequence_breakers: '"\\n", ":", "\\"", "*"'
    dynamic_temperature: False
    dynatemp_low: 1
    dynatemp_high: 1
    dynatemp_exponent: 1
    encoder_repetition_penalty: 1
    epsilon_cutoff: 0
    eta_cutoff: 0
    frequency_penalty: 0
    greeting: ''
    guidance_scale: 1
    history:
      internal: []
      visible: []
    max_new_tokens: 512
    max_tokens_second: 0
    max_updates_second: 0
    min_p: 0.00
    mirostat_eta: 0.1
    mirostat_mode: 0
    mirostat_tau: 5
    mode: 'chat'    # chat / chat-instruct / instruct
    name1: ''           # name1 and name2
    name1_instruct: ''  # are automatically
    name2: ''           # populated, so
    name2_instruct: ''  # ignore these.
    negative_prompt: ''
    no_repeat_ngram_size: 0
    penalty_alpha: 0
    presence_penalty: 0
    prompt_lookup_num_tokens: 0
    repetition_penalty: 1.18
    repetition_penalty_range: 1024
    sampler_priority: []
    seed: -1.0
    skip_special_tokens: true
    smoothing_curve: 1
    smoothing_factor: 0
    static_cache: false
    stop_at_newline: false
    stopping_strings: ''
    stream: true
    temperature: 0.98
    temperature_last: false
    tfs: 1
    top_a: 0
    top_k: 100
    top_p: 0.37
    truncation_length: 2048
    turn_template: ''
    typical_p: 1
    user_bio: ''
    xtc_threshold: 0.1
    xtc_probability: 0
    chat_template_str: >-
      {%- for message in messages %}
          {%- if message['role'] == 'system' -%}
              {{- message['content'] + '\n\n' -}}
          {%- else -%}
              {%- if message['role'] == 'user' -%}
                  {{- name1 + ': ' + message['content'] + '\n'-}}
              {%- else -%}
                  {{- name2 + ': ' + message['content'] + '\n' -}}
              {%- endif -%}
          {%- endif -%}
      {%- endfor -%}
    # The script will only initialize with this instruct template if one is not defined in the LLM model's metadata.
    instruction_template_str: >-
      {%- set ns = namespace(found=false) -%}
      {%- for message in messages -%}
          {%- if message['role'] == 'system' -%}
              {%- set ns.found = true -%}
          {%- endif -%}
      {%- endfor -%}
      {%- if not ns.found -%}
          {{- '' + 'Below is an instruction that describes a task. Write a response that appropriately completes the request.' + '\n\n' -}}
      {%- endif %}
      {%- for message in messages %}
          {%- if message['role'] == 'system' -%}
              {{- '' + message['content'] + '\n\n' -}}
          {%- else -%}
              {%- if message['role'] == 'user' -%}
                  {{-'### Instruction:\n' + message['content'] + '\n\n'-}}
              {%- else -%}
                  {{-'### Response:\n' + message['content'] + '\n\n' -}}
              {%- endif -%}
          {%- endif -%}
      {%- endfor -%}
      {%- if add_generation_prompt -%}
          {{-'### Response:\n'-}}
      {%- endif -%}
    # <|character|> gets replaced by the bot name, and <|prompt|> gets replaced by the regular chat prompt.
    chat-instruct_command: >-
      Continue the chat dialogue below. Write a single reply for the character "<|character|>".\n
      \n
      <|prompt|>